\section{Grenzen relationaler Datenbanken}

\subsection{Eingeschränkte Skalierbarkeit}

Das rasante Datenwachstum der letzten zwei Dekaden bedingte die Notwendigkeit kontinuierlich expandierender digitaler Infrastruktur, wie etwa Server und Datenbanken. Die parallel zunehmende Rechenleistung und Speicherkapazitäten einzelner Computerchips und Festplatten konnten diesem Trend nicht kompensieren, sodass die vertikale Skalierung durch Erhöhung der Rechenleistung einzelner Computer an ihre technischen und wirtschaftlichen Grenzen stieß. Um das Problem zu lösen, wurde vermehrt auf die horizontale Skalierung gesetzt, bei der mehrere physikalisch getrennte Rechenknoten zusammenarbeiten. Diese Netzwerke bilden die Grundlage heutiger Rechenzentren und Supercomputer. Die genannten Limitierungen konnten durch Parallelisierung gelöst werden und ermöglichen theoretisch unbegrenzte Kapazität. Darüber hinaus führt die Verteilung zu einer höheren Verfügbarkeit und Ausfallsicherheit. Ist ein Knoten, beispielsweise auf Grund von Wartungsarbeiten nicht erreichbar, wird die verlorene Rechenleistung von anderen Knoten ausgeglichen, ohne merkliche Auswirkungen auf die Anwendung und deren Nutzer. Relationale Datenbanken sind traditionell für die vertikale Skalierung ausgelegt, und speziell optimiert für den Betrieb auf einem einzelnen Server. Die effektive und effiziente (optimale) horizontale Skalierung relationaler Datenbanken ist aufgrund der strengen ACID-Eigenschaften und der Einschränkung des CAP-Theorems nicht, oder nur mit erheblichem Kostenaufwand, möglich. \footcite[S. 1]{schreinerWhenRelationalBasedApplications2019} Ein Ansatz war der sogenannte Memcache, wobei die häufigeren Lesezugriffe auf mehrere Replikationsserver verteilt wurden. Beim Sharding werden große Tabellen über mehrere Datenbankserver partitioniert. Es stellte sich jedoch heraus, dass diese komplizierten Ansätze nicht nur viele Nachteile und hohe Kosten verursachen, sondern die notwendige Skalierbarkeit, wie etwa von Sozialen Netzwerken oder Online-Shops benötigt, nicht erreichbar ist. \footcite[S. 41-43]{harrisonNextGenerationDatabases2015} 

\subsection{Fehlende Flexibilität \& Impedance Mismatch}

Für die Erstellung der Tabellen einer relationalen Datenbank müssen die Spalten und deren Datentypen streng definiert werden. Jede Spalte benötigt dabei einen eindeutig identifizierbaren Primärschlüssel, um die Zeilen voneinander unterscheiden zu können. Komplexe Datenmodelle werden zunächst normalisiert, um Redundanzen zu eliminieren, dabei wird das Datenmodell auf mehrere Tabellen verteilt, und miteinander via Primär- und Fremdschlüsseln verknüpft. Die vollständige Modellierung des Datenmodells kann unterstützten, um die Abhängigkeiten und Zusammenhänge zu visualisieren. Beim Einfügen eines Datensatzes, müssen die Länge genau mit der Anzahl der Spalten und den festgelegen Datentypen übereinstimmen. Ist dies nicht der Fall, lehnt das Datenbankmanagmentsystem die Daten ab, und gibt eine entsprechende Fehlermeldung zurück.

Dieser Ansatz ermöglicht neben weiteren Vorteilen eine hohe Datenkonsistenz, jedoch auf Kosten der Schema-Flexibilität. Der Ansatz der agilen Softwareentwicklung fordert einen schmalen iterativ-dynamischen Entwicklungsprozess, sodass häufige Änderungen oder Erweiterungen des Datenmodell notwendig werden. Moderne RDBMS unterstützten zwar die Modifikation des Schemas bestehender Tabellen, diese erfordern jedoch besondere Vorsicht und erfüllen nicht die geforderte einfache Flexibilität und verlässliche Stabilität, dynamischer Datenmodelle. \footcite[S. 197]{harrisonNextGenerationDatabases2015}

\footcite{newardVietnamComputerScience2006}


% Ein weiteres Problem ist das Impdance Mismatch, welcher 

% \newpage

% \subsection{Performance-Probleme bei großen Datenmengen}





% \newpage

